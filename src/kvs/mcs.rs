/*
 * Nibble - Concurrent Log-Structured Memory for Many-Core Key-Value Stores
 *
 * (c) 2017 Hewlett Packard Enterprise Development LP.
 *
 * This program is free software: you can redistribute it and/or modify it under the terms of the
 * GNU Lesser General Public License as published by the Free Software Foundation, either version 3
 * of the License, or (at your option) any later version. This program is distributed in the hope that
 * it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public License along with this program.
 * If not, see <http://www.gnu.org/licenses/>. As an exception, the copyright holders of this Library
 * grant you permission to (i) compile an Application with the Library, and (ii) distribute the Application
 * containing code generated by the Library and added to the Application during this compilation process
 * under terms of your choice, provided you also meet the terms and conditions of the Application license.
 */


/// Implementation of the MCS lock.
/// Create a shared variable of type AtomicPtr<McsQnode> as the main
/// lock.  Each threads creates a private McsQnode and passes that
/// into each of the methods lock and unlock.
use std::ptr;
use std::intrinsics;
use std::thread;
use std::sync::atomic::*;

#[inline(always)]
unsafe fn sfence() {
    asm!("sfence" : : : "memory");
}

pub struct McsQnode {
    next: AtomicPtr<McsQnode>,
    locked: AtomicBool,
}

impl McsQnode {

    pub fn new() -> Self {
        McsQnode {
            next: AtomicPtr::new(0usize as *mut Self),
            locked: AtomicBool::new(false),
        }
    }

    #[inline(always)]
    pub unsafe fn set_next(&self, next: *const Self) {
        self.next.store(next as *mut _, Ordering::SeqCst);
    }

    #[inline(always)]
    pub unsafe fn clear_next(&self) {
        self.next.store(0usize as *mut Self, Ordering::SeqCst);
    }

    #[inline(always)]
    pub unsafe fn get_next(&self) -> *mut Self {
        self.next.load(Ordering::Relaxed)
    }

    #[inline(always)]
    pub unsafe fn set_locked(&self, status: bool) {
        self.locked.store(status, Ordering::SeqCst);
    }

    #[inline(always)]
    pub unsafe fn is_locked(&self) -> bool {
        self.locked.load(Ordering::Relaxed)
    }

    pub unsafe fn lock(glob: &AtomicPtr<Self>, slot: &mut Self) {
        slot.clear_next();
        sfence(); // TODO necessary?
        let next_qnode = glob.swap(slot as *mut Self, Ordering::SeqCst);
        if !next_qnode.is_null() {
            slot.set_locked(true);
            sfence();
            (&*next_qnode).set_next(slot as *mut Self);
            while slot.is_locked() { ; }
        }
    }

    pub unsafe fn unlock(glob: &AtomicPtr<Self>, slot: &mut Self) {
        let mut next_qnode = slot.get_next();
        if next_qnode.is_null() {
            let expected = slot as *mut Self;
            let ok = expected == glob.compare_and_swap(expected,
                                0usize as *mut Self, Ordering::SeqCst);
            if ok {
                return;
            } else {
                next_qnode = slot.get_next();
                while next_qnode.is_null() {
                    next_qnode = slot.get_next();
                }
            }
        }
        (&*next_qnode).set_locked(false);
    }
}
